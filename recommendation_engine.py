# -*- coding: utf-8 -*-
"""
Created on Wed Nov 18 11:34:10 2020

@author: conno
"""
import pandas as pd
from gensim.models.doc2vec import Doc2Vec
import numpy as np

def clean_features(df,keep=['syllables_per_line','syllables_variance','lexical_diversity','sentiment','syllables_per_second'],std_drop=3):
    """
    This function will get our feature data cleaned and ready to compute similarities.

    Parameters
    ----------
    df : pandas dataframe
        dataframe consisting of our features as columns and song_ids as indexes.
    keep : List, optional
        List of features we will keep for computing similarities.
        The default is ['syllables_per_line','syllables_variance','lexical_diversity','sentiment','tokens_per_second'].
    std_drop : int, optional
        Drop rows that have features >std_drop deviations from our mean. The default is 3.

    Returns
    -------
    Pandas dataframe.

    """
    
    df = ((df - df.mean())/df.std()) #standardize



    df = df[keep]
    
    df = df[~(abs(df)>3).any(1)] #remove songs with strange features
    
    #If lexical diversity is nan, drop from our songs, might be empty or full of symbols
    df = df.dropna(subset=['lexical_diversity']) 
    
    return df

def feature_similarity(song_id,df,time_exclusion=True,time_columns=['syllables_per_second'],std_drop=3):
    """
    This function computes similarity between songs based on their euclidean distance from one another.
    The code assumes that the dataframe is standardized and rows with values greater than 5 stds from the mean are dropped.
    
    Parameters:
        
    song_id : int
        unique song_id numbe
        
    df: pandas dataframe
        standardized with rows containing values >5 stds away from mean dropped
        
    time_exclusion: Bool. 
        Exclude songs where their duration could not be identified. 
        If false, we drop features related to duration (time_columns)
                    
    time_columns: list 
        column names related to time-based features. 
        
    std_drop: int
        Same int used in clean_features, we will use this to determine the maximum distance possible in our set of vectors.
    
    Returns:
    similarity_pairs: list
        List of tuples. For x in similarity_pairs, x[0]==song_id and x[1]==similarity score
    
    """
    #If we want to include songs that we don't know their duration, drop time-dependent features
    if df[df.index==song_id][time_columns[0]].isna().values[0]:
        time_exclusion=False
        

    if time_exclusion==True: 
        df = df.dropna(subset=time_columns)
    else:
        df = df.loc[:, ~df.columns.isin(time_columns)]
        
    song_vec = df[df.index.isin([song_id])].to_numpy()
    songs_matrix = df[~df.index.isin([song_id])]
    songs_ids = list(songs_matrix.index)
    songs_matrix=songs_matrix.to_numpy()
    
    num_dims=songs_matrix.shape[1]
    max_distance = np.sqrt(num_dims*(std_drop**2)) #compute max distance to scale our similarities
    distances = np.sqrt(np.square(songs_matrix-song_vec) @ np.ones(num_dims)) #compute euclidean distance
    similarities = (max_distance - distances)/max_distance #low distance -> high similarity
    
    similarity_pairs = [(idx,sim) for idx,sim in sorted(zip(songs_ids,similarities),reverse=True, key=lambda pair: pair[1])]
    return similarity_pairs
   
def similarity_df(feature_similarity,doc_model,song_id,feature_importance=0.5,doc_importance=0.5, num_songs=1000000000):
    """
    

    Parameters
    ----------
    feature_similarity : List
        List of tuples generated by feature_similarity()
    doc_model : doc2vec model
        Trained doc2vec model
    song_id : int or str
        song_id used to compute similarity in feature_similarity()
    feature_importance : float, optional
        default is 0.5, final weighting that is applied to our combined similarity rating. sum(feature_importance,doc_importance) should equal 1
    doc_importance : float, optional
        default is 0.5, final weighting that is applied to our combined similarity rating. sum(feature_importance,doc_importance) should equal 1
    num_songs : int optional
        number of similarity scores to return from the doc2vec model, defaults to huge number to get all

    Returns
    -------
    result: Pandas dataframe
        sorted (descending) pandas dataframe
    

    """
    m = doc_model.docvecs.most_similar(str(song_id),topn=num_songs)
    
    m=pd.DataFrame(m,columns=['id','doc_similarity'])
    m.loc[:,'id'] = pd.DataFrame(m)['id'].astype(int)
    m = m.set_index('id')
    
    s = pd.DataFrame(feature_similarity,columns=['id','feature_similarity']).set_index('id')
    
    result = pd.concat([s,m],axis=1)
    result.dropna(inplace=True)
    
    result = (result - result.min()) / (result.max() - result.min()) #min max scale so doc similarity and feature_similarity are of equal importance
    result['combined']=(0.5*result.feature_similarity+0.5*result.doc_similarity)
    result = result.sort_values('combined',ascending=False)
    
    return result

def decoder(df):
    """
    

    Parameters
    ----------
    df : Pandas dataframe
        pandas dataframe containing all song ids, artist, song title, etc.

    Returns
    -------
    decode_dict: dictionary
        dictionary to decode song_ids into song title and artist

    """
    song_ids = list(df.song_id)
    artists = list(df.artist_name)
    song_titles = list(df.song_title)
    
    decode_dict={i:{'artist':a,'song':s} for i,a,s in zip(song_ids,artists,song_titles)}
    
    return decode_dict
    
def recommend(df,decoder,song_id=None,n=5):
    df = df.iloc[:n,:]
    artists = list(pd.Series(df.index).apply(lambda x: decoder[x]['artist']))
    songs = list(pd.Series(df.index).apply(lambda x: decoder[x]['song']))
    
    if song_id:
        print('Recommendations for '+decoder[song_id]['song']+' by '+decoder[song_id]['artist'])
        print('')
    
    for i in range(n):
        print('Try out \''+songs[i]+'\' by '+artists[i])
        
    return